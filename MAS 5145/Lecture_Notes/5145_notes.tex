\documentclass{report}

\input{/Users/Nick/Desktop/UCF/UCF_Notes/.vscode/preamble}
\input{/Users/Nick/Desktop/UCF/UCF_Notes/.vscode/macros}
\input{/Users/Nick/Desktop/UCF/UCF_Notes/.vscode/letterfonts}

\title{MAS 5145 Lecture Notes - Fall 2024}
\author{Nickolas Arustamyan}
\date{\today}

\begin{document}
\maketitle
\tableofcontents
\chapter{Review}
\section{08/20/2024}
\dfn{Vector Spaces}{A Vector Space is a nonempty set V with two operations, vector addition and scaler multiplication. These operations must satisfy a bunch of axioms, most important of which is $u,v \in V \implies u+v \in V$ and for $\alpha \in \mathbb{F}, \alpha v \in V$.}
\dfn{Subspace}{A Subspace W of a vector space V is a nonempty subset of V with the same operations as V. }

\mprop{}{The intersection of any collection of subspaces $W_j$ of V is itself a subspace of V}
\begin{myproof}
In order to be a subspace, we must prove that the intersection is nonempty and that it is closed under the operations of V. Clearly, since each $W_j$ is a subspace, they must each contain the zero element. Hence, the intersection must as well and hence, the intersection is nonempty. For any elements $u, v \in \bigcap W_j = W$, we know that a linear combination $\alpha u + \beta v \in W_j$ for each $W_j$ since they are each subspaces and hence closed under the vector operations. This means that $\alpha u + \beta v \in W$ and hence $W$ is a subspace.
\end{myproof}

\dfn{Direct Sum}{Given two subspaces $W_1$ and $W_2$ of $V$, if $W_1\cap W_2 = \{0\}$, then $W_1+W_2$ is a direct sum of $W_1$ and $W_2$. For a collection of subspaces, we have a direct sum of $W_i \bigcap_{j \neq i} W_j = \{0\}$ }

\section{08/22/2024}
\dfn{Linear Combination}{Let $V$ be a vector space and $B = \{v_1, ..., v_k\} \subset V$. A linear combination of $B$ is a vector of the form $v = \sum c_iv_i (c_i \in \mathbb{F}) $. }

\dfn{Spanning Set}{$S \subseteq V$ is called a spanning set if $span(S) = V$. }

\dfn{Linear Independence and Dependence}{$B\subset V$ is called linearly dependent if there exists $c_1, ..., c_n$ not all $0$ such that $\sum b_i c_i = 0$. Otherwise we say that $B$ is linearly independent.}

\dfn{Basis}{$S \subseteq V$ is called a basis of $V$ if it is linearly independent and spanning.}

\thm{}{Every Vector Space has a basis}
\begin{myproof}
    In the finite dimensional case, we know that $V = span(\{v_1, ..., v_n\})$ for some set $v_1,..., v_n$. If the spanning set is linearly independent, then we have a basis. Otherwise, remove linearly dependent vectors and recheck until we have a linearly independent set, which is thus a basis. In the infinite dimensional case, we must use Zorns Lemma but it is true that we can find the basis.
\end{myproof}

\qs{}{Every spanning set contains a basis}
\begin{myproof}
    Let $B = \{v_1, ..., v_n\}$ such that $B$ is a spanning set. If $B$ is linearly independent, the basis is itself. Otherwise, there must be some vector $v_k$ that can be written as a linear combination of the other vectors. We can remove $v_k$ and recheck the new $B$ to see if it is linearly independent. This process must terminate and when it does, the final set will be linearly independent by construction. Hence, that final set will be a basis.
\end{myproof}

\qs{}{Every linearly independent set can be extended to a basis}
\begin{myproof}
    Let $B = \{v_1, ..., v_n\}$ such that $B$ is linearly independent. If $span(B) = V$, then we have a basis. Otherwise, there must be some vector $v \in V$ such that $v \not\in span(B)$. Append $v$ to $B$ and recheck if it is a spanning set. If not, repeat the process until we have a spanning set. At that point we will have a basis.
\end{myproof}

\qs{}{Suppose $A = \{v_1, ... , v_k\}$ is linearly independent and $B = \{w_1, ... , w_m\}$ is a spanning set. Then $k \leq m$. }
\begin{myproof}
    We know that every linearly independent set can be extended to form a basis. This means that one can turn the LI set into one that also spans only by adding vectors to it. Similarly, every spanning set contains a basis implies that one can turn a spanning set into one that also is LI only by removing vectors from it. Together, these imply that the cardinality of any spanning set must be greater than or equal to that of any LI set. Hence, $k \leq m$.
\end{myproof}

\dfn{Dimension of a Vector Space}{Let $S$ be a basis for a vector space $V$. Then $dim(V)$ is the cardinality of $S$.}

\mlenma{}{Let $dim(V) = n < \infty$. Every $n$ LI vectors form a basis.}
\begin{myproof}
    Let $\{v_1, v_2, ..., v_n\}$ be a LI set. Then we can extend it to a basis $B = \{v_1, v_2, ..., v_n, u_1, ..., u_k\}$. But we know that the dimension of $V$ is $n$ and since $B$ is a basis of $V$, then $dim(V) = n+k$. Hence $n = n+k$ which implies $k = 0$ and the original set was a basis. 
\end{myproof}

\mlenma{}{Let $dim(V) = n < \infty$. Every $n$ spanning vectors form a basis}
\begin{myproof}
    Let $\{v_1, v_2, ..., v_n\}$ be a spanning set. Then we can select vectors from it to form a basis $B = \{v_{i_1}, v_{i_2}, ..., v_{i_k} \}$. But we know that the dimension of $V$ is $n$ and since $B$ is a basis of $V$, then $dim(V) = k$. Hence $n = k$ which implies the original set was a basis.
\end{myproof}
If $B$ is a basis of $V$, then every vector in $V$ has a unique representation as a linear combination of vectors in $B$. 
\section{08/27/2024}
Let $W_1, W_2$ be subspaces of $V$. Then the following are equivilant 
\begin{enumerate}
    \item $W_1 \oplus W_2$
    \item For all $v\in W_1 + W_2$ there exists unique $w_1 \in W_1, w_2 \in W_2$ such that $v = w_1 + w_2$
    \item $w_1 + w_2 = 0 \implies w_1 = w_2 = 0$
    \item The union of a basis for $W_1$ one for $W_2$ is basis of $W_1 + W_2$
\end{enumerate}

\begin{myproof}
    The first three all follow from definition of the direct sum. The fourth equivilance can be seen by letting $A = \{u_1, u_2, ..., u_k\}$ be a basis for $W_1$ and $B = \{v_1, v_2, ..., v_n\}$ be a basis for $W_2$. Clearly, $span(A\cup B) = W_1+W_2$. Linear indepence can be seen since the two sets themselves are basis and hence linearly independent. Thus, we have a basis of $W_1 + W_2$. 
\end{myproof}

\mlenma{}{If $W$ is a subspace of $V$, then there exists a subspace $U$ of $V$ such that $V = U \oplus W$. $U$ is called a complement of $V$. }
\begin{myproof}
    Let $\{w_1, w_2, ..., w_k\}$ be a basis of $W$. Extend it to be a basis for $V, \{w_1, ..., w_k, u_1, ..., u_n\}$. Set $U = span(\{u_1, ..., u_n\})$. Thus, $V = U \oplus W$.
\end{myproof}

\thm{Dimension Formula for Subspaces}{Let $W, U$ be finite dimensional subspaces of $V$. Then $dim(W+U) = dim(W) + dim(U) - dim(W \cap U)$.}
\begin{myproof}
    Let $B_1 = \{v_1, ..., v_n\}$ be a basis for $W\cap U$. Then we can extend $B_1$ to form a basis of $W$ and get $B_2 = \{v_1, ..., v_n, w_1, ..., w_k\}$. Similarly, we can extend $B_1$ to form a basis for $U$ and get $B_3 = \{v_1, ..., v_n, u_1, ..., u_l\}$. Thus $dim(W) + dim(U) + dim(W \cap U) = n + k + n + l - n = n+k+l$. Now, we claim that $B = \{v_1, ..., v_n, w_1, ..., w_k, u_1, ..., u_l\}$ is a basis for $W+U$. Clearly, $span(B) = W+U$. To verify linear independence, we first set $\sum a_i v_i + \sum b_j w_j + \sum c_m u_m = 0$. This means that $-\sum a_i v_i = \sum b_j w_j + \sum c_m u_m$. Since the left hand side is in $W$ and the right hand side is in $U$, then they must  be in the intersction. This means that there are some $d_i \in \mathbb{F}$ such that $\sum d_i v_i = -\sum c_m u_m$. Since $B_1$ is a linearly independent set, this implies that $c_m = 0$. From this, it follows $a_i, b_j = 0$ and hence the set $B$ is linearly independent and a basis. Thus, $dim(W+U) = n+k+l$. 
\end{myproof}

\dfn{Linear Transformation}{Let $V, W$ be two vector spaces over $\mathbb{F}$. A map $\alpha: V \rightarrow W$ is linear if $\alpha(au+bv) = a\alpha(u) + b\alpha(v)$}

\mprop{}{If $\alpha$ is linear, then:
\begin{itemize}
\item $ker(\alpha) = \{ v\in V : \alpha(v) = 0\}$ is a subspace of $V$.
\item $\alpha(0) = 0$
\item $im(\alpha) = \{\alpha(v) : v\in V  \}$ is a subspace of $W$.
\end{itemize}}
\begin{myproof}
    NEED TO DO
\end{myproof}

\dfn{Injective or One-to-One}{A linear transformation $\alpha$ is called injective or one to one if $\alpha(x) = \alpha(y) $ if and only if $x=y$}

\qs{}{A linear transformation $\alpha$ is injective if and only if $ker(\alpha) = \{0\}$. }


\begin{myproof}
    Assume that $\alpha$ is injective. Now, suppose that $x \in ker(\alpha)$. This means that $\alpha(x) = 0$. But we know that $\alpha(0) = 0$. Since $\alpha$ is injective and $\alpha(x) = \alpha(0) = 0$, then $x = 0$. Hence $ker(\alpha) = \{0\}$. This has proved one direction. For the other direction, assume that $ker(\alpha) = \{0\}$. Now assume that $\alpha(x) = \alpha(y)$. This means that $\alpha(x) - \alpha(y) = 0$ and hence $x- y \in ker(\alpha)$. But this implies that $x=y$. Hence, when $\alpha(x) = \alpha(y)$, $x=y$. Thus, $\alpha$ is injective. 
    \end{myproof}


\dfn{Surjective or onto}{A linear transformation $\alpha$ is called surjective or onto if $im(\alpha) = W$.}

\dfn{Bijective}{A linear transformation $\alpha$ is called bijective it is both injective and surjective.}

\mprop{}{Let $\alpha: V \rightarrow W$ be linear:
\begin{enumerate}
\item $\alpha$ is injective if and only if $ker(\alpha) = 0$
\item If $\alpha$ is bijective, $\alpha^{-1}$ is linear
\item If $\alpha$ is injective and $S$ is LI, then $\alpha(S)$ is LI
\item If $\alpha$ is surjective and $span(S) = V, span(\alpha(S)) = W$. 
\item If $\alpha$ is bijective, $\alpha$ maps a basis of $V$ to a basis of $W$. 
\end{enumerate}}


\begin{myproof}
    NEED TO DO
\end{myproof}

\mprop{}{If $\alpha: V \rightarrow W$ and $\beta: W \rightarrow U$ are both linear, then so is $\beta\alpha$. }

\begin{myproof}
    NEED TO DO
\end{myproof}

\dfn{Nullity}{We define $nullity(\alpha) = dim(ker(\alpha))$.
}

\dfn{Rank}{We define $rank(\alpha) = dim(im(\alpha))$.
}

\thm{Dimension Formula for Linear Transformations}{If $V$ and $W$ are finite dimensional vector spaces, then if $\alpha$ is linear, $dim(V) = rank(\alpha) + nullity(\alpha)$.}

\begin{myproof}
    Since $ker(\alpha)$ is a subspace, we know it must have a basis. Let $B = {v_1, v_2, ..., v_k}$ be a basis for $ker(\alpha)$. We know that the kernal is finite dimensional since it is a subspace of a finite dimensional space $V$. Now, extend $B$ to form a basis for $V$. Let $dim(V) = n$. Hence, we get $A = {v_1, ..., v_k, v_{k+1}, ..., v_n}$. Now, $im(\alpha) = span(\alpha(A)) = span(\alpha(v_1), ..., \alpha(v_k), \alpha(v_{k+1}), ..., \alpha(v_n)) = span(\alpha(v_{k+1}), ..., \alpha(v_n)) = span(A \setminus B)$. We now know that $A\setminus B = C$ is a spanning set of $im(\alpha)$. To see that $C$ is linearly independent, we set $\sum_{i=1}^{n-k} d_i\alpha(v_i) = 0$. Since $\alpha$ is linear, we can take out the transformation and get $\alpha(\sum_{i=1}^{n-k} d_iv_i) = 0$. But this means that $\sum_{i=1}^{n-k} d_iv_i \in ker(\alpha)$ which cannot be the case unless all $d_i = 0$. Hence, $C$ is linearly independent and thus a basis for $im(\alpha)$. Thus $rank(\alpha) + im(\alpha) = k + (n-k) = n = dim(V)$.
\end{myproof}

\section{08/29/2024}

\mlenma{}{Suppose $dim(V) = dim(W) = n < \infty$ and $T:V \rightarrow W$ is a linear map. Then $T$ is injective if and only if it is surjective.}

\begin{myproof}
    Assume that $T$ is injective. Then $ker(T) = \{0\}$ and hence $nullity(T) = 0$. Thus, by the Dimension Formula for Linear Transformations, we know that $rank(T) = dim(V) - nullity(T) = n-0=n$. Hence $rank(T) = n$ and thus, $T$ is surjective. Now, assume instead that $T$ is surjective. Similarly, we know that $nullity(T) = 0$ and thus, $T$ is injective. 
\end{myproof}

\thm{}{    Say $T:V \rightarrow W$ and $S:W \rightarrow Y$ with $T, S$ linear. Then 
\begin{enumerate}
    \item $nullity(ST) \leq nullity(T) + nullity(S)$
    \item $rank(T) + rank(S) - dim(W) \leq rank(ST) \leq  min(rank(S), rank(T))$
\end{enumerate}}

\begin{myproof}
    Since $ker(ST)$ is a subspace, we know it has a basis and hence, let $B = \{c_1, ..., c_g\}$ to be a basis for the kernal. This is the set of all vectors $c\in V$ such that $S(T(c)) = 0$. These specific $T(v)$ form a subset of $ker(S)$. Call that set $G$. Then $G \subseteq ker(S)$ and hence $ker(ST) \subseteq ker(S)$. This implies that $nullity(ST) \leq nullity(S) \leq nullity(S) + nullity(T)$.
    \newline
    To prove the second item, NEED TO DO
\end{myproof}

\dfn{Homomorphisms}{Let $V, W$ be two vector spaces. The set of all linear transformations from $V$ to $W$ is called $Hom(V,W)$ or $L(V, W)$. }
\mclm{}{$Hom(V, W)$ is a vector space and $dim(Hom(V, W)) = dim(V)\cdot dim(W)$.}

\end{document}